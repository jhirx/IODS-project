---
author: Jouni Hirvonen
title: "Logistic regression"
output: html_document


# Logistic regression

Code for data creation is available at:   
  https://github.com/jhirx/IODS-project/blob/master/data/chapter3.R

reading data in
```{r}
alc <- read.table("~/IODS-project/data/alc_table.csv", sep=",", header=TRUE)
library(dplyr); library(ggplot2)
```


My data  (alcohol high use to explaning internet , romatic and sex = Gender )

```{r}
#my hypothese  (alcohol high use to explaning internet , romatic and sex = Gender )


m <- glm(high_use ~ internet + romantic + sex, data = alc, family = "binomial")

# print out a summary of the model where only sex have some signif 
summary(m)
#pairs(m)

# print out the coefficients of the model
coef(m)



# compute odds ratios (OR)
OR <- coef(m) %>% exp

# compute confidence intervals (CI)
CI <- confint(m) %>% exp

# print out the odds ratios with their confidence intervals , where same result only SEX influense alcohol use
cbind(OR, CI)

```
setting and calculatining probability and prediction (internet, romantic, sex, high_use, probability, prediction)
```{r}

# predict() the probability of high_use
probabilities <- predict(m, type = "response")

# add the predicted probabilities to 'alc'
alc <- mutate(alc, probability = probabilities)

# use the probabilities to make a prediction of high_use
alc <- mutate(alc, prediction = probability > 0.5)

# see the last ten original classes, predicted probabilities, and class predictions
#and we see probability and prediction is LOW
select(alc, internet, romantic, sex, high_use, probability, prediction) %>% tail(10)


# tabulate the target variable versus the predictions
table(high_use = alc$high_use, prediction = alc$prediction)

# initialize a plot of 'high_use' versus 'probability' in 'alc'
g <- ggplot(alc, aes(x = probability, y = high_use, col = prediction))

# define the geom as points and draw the plot , tells something??
g + geom_point()


#qplot(internet, romantic, high_use = alc) + geom_smooth(method = "lm")
# tabulate the target variable versus the predictions True und 0.5 so no prediction
table(high_use = alc$high_use, prediction = alc$prediction) %>% prop.table %>% addmargins

#these my chosen variables have not relationships with alcohol consumption

# define a loss function (mean prediction error)
loss_func <- function(class, prob) {
  n_wrong <- abs(class - prob) > 0.5
  mean(n_wrong)
}

# call loss_func to compute the average number of wrong predictions in the (training) data
# 0,29... tells prediction is right
loss_func(class = alc$high_use, prob = 0)



```

Cross-validation is a method of testing a predictive model on unseen data. 
In cross-validation, the value of a penalty (loss) function (mean prediction error)
is computed on data not used for finding the model. Low value = good.

Cross-validation gives a good estimate of the actual predictive power of the model. 
It can also be used to compare different models or classification methods.


```{r}

# define a loss function (average prediction error)
loss_funca <- function(class, prob) {
  n_wrong <- abs(class - prob) > 0.5
  mean(n_wrong)
}

# compute the average number of wrong predictions in the (training) data, and perdiction
loss_funca(class = alc$high_use, prob = alc$probability)

# K-fold cross-validation
library(boot)
cv <- cv.glm(data = alc, cost = loss_func, glmfit = m, K = 10)

# average number of wrong predictions in the cross validation
cv$delta[1]
```

