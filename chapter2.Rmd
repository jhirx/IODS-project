<<<<<<< HEAD
# Regression and model validation 

My try to make same Markdown document

## Reading data

Code for data creation is available at:   
  https://github.com/jhirx/IODS-project/blob/master/data/create_learning2014.R

Let's read the data in and make sure that gender is converted to factor
```{r}
##setwd("~/IODS-project")
library(dplyr)
#learning2014 <-read.csv("~/IODS-project/data/learning2014.csv", sep="/t", header=TRUE)
learning2014 <- read.csv("d:/yliopisto/IODS-project/data/learning2014.csv") %>%
#learning2014 <- read.csv("~/IODS-project/data/learning2014.csv") %>%
 mutate_at(vars(gender), factor)
```

how data structure looks like
```{r datastructure}
str(learning2014)
```

## Exploring data

Here some figures to see how the data looks
```{r fig1, fig.path="figures/"}
pairs(learning2014[!names(learning2014) %in% c("gender")],col=learning2014$gender)
```

```{r fig2, fig.path="figures/", fig.dim=c(10,10), results='hide', message=FALSE}
library(GGally)
library(ggplot2)
# create a more advanced plot matrix with ggpairs()
ggpairs(learning2014, 
        mapping = aes(col = gender, alpha = 0.3), 
        lower = list(combo = wrap("facethist", bins = 20))
)
```



## Linear regression

The highest correlation is between *attitude* and *points*, **Cor:** `r cor(learning2014$attitude,learning2014$points)`.
Let's take a closer look.

```{r}
qplot(attitude, points, data = learning2014) + geom_smooth(method = "lm")
```

Let's fit a linear model to the data. Points are explained by attitude.
The equation for the model is
$$
  Y_i = \alpha + \beta_1 X_i + \epsilon_i
$$
  where Y represent points, X is attitude, $\alpha$ is constant, $\beta_1$ is regression
coefficient for attitude, and $\epsilon$ is a random term.

Estimation of the model yields the following results:
  ```{r, results='asis'}
my_model <- lm(points ~ attitude, data = learning2014)
results <- summary(my_model)
knitr::kable(results$coefficients, digits=3, caption="Regression coefficients")
```

Intercept as well as attitude are statistically significant predictors. 
Coefficient of determination $R^2$ = `r results$r.squared` that is not particularly high.
Probably some more predictors could be added to the model.

### Diagnostic plots
#```{r fig3, fig.path="figures/"}
#plot(my_model, which=c(1,2,5))
#>>>>>>> e8ec3ad5cf6516668bd08875ce443453e4e64fd3
